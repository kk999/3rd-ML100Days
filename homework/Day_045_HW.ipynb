{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "三篇文獻，我選了「完整的 Ensemble 概念 by 李宏毅教授」，記錄一些心得於此。\n",
    "\n",
    "Ensemble，是一種團隊合作，好幾個模型一起上的方法，\n",
    "這些要一起團隊合作的模型，我們會希望是Diverse的，\n",
    "就是說，每個模型都有各自與眾不同的屬性，自己的任務目標。\n",
    "\n",
    "Ensemble通常是用在需要更進一步取得成果的時候，\n",
    "因為只使用一種模型得到的成果，通常是不如一群模型一起得到的成果。\n",
    "\n",
    "復習：Bias與Variance\n",
    "在做ML(Machine Learning)的時候有Bias和Variance的trade-off，\n",
    "一個簡單的模型，不容易fit資料集，所以會有大的Bias，小的Variance，\n",
    "一個複雜的模型，太容易fit資料集，所以會有小的Bias，大的Variance，\n",
    "因此隨著模型從簡單到複雜來看，error rate會逐漸變小然後再漸漸增加，\n",
    "\n",
    "所以如果只要取一個最好的模型的話，可以取error rate最小的那個，\n",
    "另也可以綜合所有模型的輸出，比如用取平均的方式，產生一個新的模型，\n",
    "這個新的模型，有可能會跟正確答案是最接近的，而這就是Ensemble。\n",
    "\n",
    "Ensemble，主要有兩種策略，Bagging及Boosting，\n",
    "兩者的差別在於，各個模型之間的關係，\n",
    "Bagging的各個模型之間是彼此獨立的；\n",
    "Boosting則是相依於前一個模型的。\n",
    "\n",
    "Bagging：\n",
    "因為各模型彼此獨立，所以相對於Boosting，\n",
    "Bias沒什麼太大的改善，但Variance卻可以得到收斂。\n",
    "也因此各個模型的參數調校重點在於降低Bias，\n",
    "比如說使用決策樹的話，就讓深度相對於Boosting來講，再深一點。\n",
    "\n",
    "Boosting：\n",
    "因為後一模型主要是用來改進前一個模型的成果，\n",
    "所以相對於Bagging來講，Bias會依模型的數量來逐漸變好，\n",
    "所以模型決策樹的深度就可以相對的淺些，\n",
    "並也可以考慮提考模型的數量來改進Bias。\n",
    "\n",
    "Stacking:\n",
    "不論Bagging或是Boosting集合不同模型的成果的方法，都可以直接簡單地用平均值或投票來決定最終的結果，\n",
    "但Stacking卻是把各模型的輸出，當成一種模型的輸入，然後取輸出來當成最後的結果。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
